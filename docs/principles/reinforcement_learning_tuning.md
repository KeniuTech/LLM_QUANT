# 强化学习调参与决策优化原理

本篇整理多智能体投资助理中使用强化学习与黑箱优化改进决策流程的核心思路，涵盖环境建模、动作设计、奖励函数、训练流程与评估方法。

## 总体目标

- 在既有规则型代理与部门 LLM 框架上，引入可迭代的策略搜索机制，最大化收益并控制风险。
- 通过可重复的回测闭环记录数据、提示参数与执行结果，为强化学习/贝叶斯优化提供高质量轨迹。
- 关注高层策略调度（Prompt、温度、投票权重、function 调用），而非直接预测行情，保持模型解释性。

## 环境建模

- **基础环境**：`app/backtest/engine.py` 提供日频循环，将代理决策、持仓状态与行情数据解耦。
- **DecisionEnv**：封装成 Gym 风格接口，`step()` 接受策略动作，运行回测并返回奖励与观察。
- **状态设计**：拼接市场特征、历史动作、LLM 置信度、风险事件、日志指标，可进一步用 RNN/Transformer 编码。
- **动作空间**：包含连续参数（温度、权重、阈值）与离散选择（Prompt 模板、协作模式、function 策略）；必要时拆分为多头动作向量。

## 奖励函数

- 基础指标：净值增长、最大回撤、换手率、成交成本。
- 风险约束：将仓位超限、冲突次数、风险事件纳入惩罚项，可采用拉格朗日或 penalty 方法。
- 信息一致性：对 LLM 置信度一致性、提示调用成本等补充奖励/惩罚，提高策略可解释性与效率。

## 训练策略

1. **数据记录**：在 `agent_utils`、`tuning_results` 等表中写入 Prompt 版本、温度、function 调用次数与回测指标，形成可复用轨迹。
2. **黑箱优化基线**：先用 Bandit、CMA-ES、贝叶斯优化等方法探索高价值参数区域，为 RL 提供 warm start。
3. **强化学习算法**：在多步 `DecisionEnv` 上使用 PPO/SAC/DDPG 等连续动作算法，必要时采用分层或多智能体（CTDE）训练。
4. **离线/离策略训练**：利用历史轨迹进行 CQL、IQL 等离线 RL；对分布偏移显著的场景考虑模型驱动或模拟生成数据。
5. **安全约束**：训练过程中强制执行仓位/风险限制，确保策略不会违反合规规则。

## 评估与对比

- **回测指标**：收益、回撤、夏普、Sortino、换手、成交成本、超额收益稳定性。
- **超参敏感度**：记录不同 Prompt/温度组合的表现，形成雷达图或分布可视化。
- **实验管理**：所有实验写入 `tuning_results`，支持在 UI 中按实验 ID 对比。
- **上线前验证**：离线回测 → 滚动前向测试 → 影子运行 → 小规模实盘，逐步扩大影响范围。

## 实施建议

- 先补齐日志字段和数据记录，确保每次实验可重现。
- 在单个部门或少量参数上做 PoC，验证闭环后再推广。
- 将 RL 与规则策略结合：RL 提供权重建议，规则层负责安全边界。
- 持续复盘失败案例，迭代奖励函数与状态设计。

后续如扩展新的动作维度或奖励项，请同步更新本原理文档，保持训练框架透明可追踪。
